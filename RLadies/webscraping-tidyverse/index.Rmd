---
title: "Webscraping with<br>`tidyverse`<br>Packages<br>"
author: "<br>Sam Tyner<br>(co-organizer R-Ladies Ames)"
date: "9 Feb 2017"
output:
  revealjs::revealjs_presentation:
    css: rladies_revealjs.css
    highlight: pygments
    transition: slide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', message = FALSE, warning = FALSE)
```

```{r rladiestheme, echo = FALSE}
r_ladies_theme <- function(){
  theme_bw() %+replace% 
    theme(text = element_text(family = "HelveticaNeue", face = "plain",
                              colour = 'black', size = 10,
                              hjust = .5, vjust = .5, angle = 0, 
                              lineheight = 1.1, 
                              margin = margin(t = 0, r = 0, b = 0, l = 0, 
                                              unit = "pt"), 
                              debug= FALSE), 
          axis.text = element_text(colour = "#181818"), 
          axis.title = element_text(face = "bold", colour = "#88398A", size = rel(1.1)), 
          plot.title = element_text(face = "bold", size = rel(1.4), 
                                    colour = "#88398A", 
                                    margin = margin(t = 0, r = 0, b = 6.6,
                                                    l = 0, unit = "pt")),
          legend.title = element_text(face = "bold", colour = "#181818"),
          panel.grid.major = element_line(color = "#D3D3D3"))
}
```

## Outline 

1. Introduction
    + What is webscraping? 
    + Why webscrape? 
2. Webscraping in `R`
    + Available packages (other than `tidyverse`)
3. The `tidyverse`?
    + [The tidy tools manifesto](https://cran.r-project.org/web/packages/tidyverse/vignettes/manifesto.html)
    + You may already know
    + Packages for the web
4. `rvest` quick start guide
    + Your Turn #1
5. Deeper dive into `rvest`
    + Key functions
    + Your Turn #2
6. Advanced Examples

# Introduction

## What is webscraping? 

- Extract data from websites 
    + Tables
    + Links to other websites
    + Text

```{r echo=FALSE, out.width='33%', fig.show='hold', fig.align='default'}
knitr::include_graphics(c('./images/gdpss.png','./images/cropsss.png','./images/gass.png'), auto_pdf = FALSE)
```    
    
## Why webscrape? 

>- Because copy-paste is awful 
```{r echo=FALSE, out.width='50%'}
knitr::include_graphics("./images/copypastesucks.png", auto_pdf = FALSE)
```
>- Because it's fast
>- Because you can automate it

# Resources for<br>Webscraping in `R`

## `R` Packages<br>for Webscraping

Lots to choose from: `XML`, `XML2R`, `scrapeR`, `selectr`, `rjson`, `RSelenium`, etc. 

Many more (and links to the above) on the [Web Technologies CRAN Task View](https://cran.r-project.org/web/views/WebTechnologies.html)

But, we'll be using the `tidyverse` packages `rvest` and `xml2`

# What is the `tidyverse`?

## The Tidy Tools [Manifesto](https://cran.r-project.org/web/packages/tidyverse/vignettes/manifesto.html)

> "The tidyverse is a set of packages that work in harmony.... The tidyverse package is designed to make it easy to install and load core packages from the tidyverse in a single command." - [RStudio Blog](https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/)

>1. Reuse existing data structures. (i.e. stick with data frames!)
>2. Compose simple functions with the pipe. (Each function does one simple thing well.)
>3. Embrace functional programming. (OOPers may find this difficult. If you are totally lost, you'll be fine.)
>4. Design for humans. (Code should be understood by humans first, then computers)

## Familiar Friends

You may already have used: 

- `ggplot2` for visualization
- `dplyr` for data manipulation
- `tidyr` for data tidying 

Install all tidyverse packages in one fell swoop: 
```{r gettv, echo = T, eval = FALSE}
# check if you already have it
library(tidyverse)
# if not:
install.packages("tidyverse")
library(tidyverse) # only calls the "core" of tidyverse
```

## `tidyverse` packages<br>for web data

- `httr`: for web APIs (Application Programming Interface) 
- `jsonlite`: for JSON (JavaScript Object Notation) data from the web
- `xml2`: for XML (eXtensible Markup Language) structured data
- `rvest`: package of wrapper functions to `xml2` and `httr` for easy web scraping

We'll focus on `rvest`

# Webscraping with `rvest`:<br>Step-by-Step Start Guide

## Step 1: Find a URL

What data do you want? 

>- Information on Oscar-nominated film Moonlight

Find it on the web! 

>- [IMDB page](http://www.imdb.com/title/tt4975722/)
```{r url}
# character variable containing the url you want to scrape
myurl <- "http://www.imdb.com/title/tt4975722/"
```

## Step 2: Read HTML into `R`

> "Huh? What am I doing?" - some of you right now  

>- HTML is HyperText Markup Language. All webpages are written with it.
>- Go to any [website](http://www.imdb.com/title/tt4975722/), right click, click "View Page Source" to see the HTML

```{r gethtml, message = FALSE}
library(tidyverse)
library(rvest)
myhtml <- read_html(myurl)
myhtml
```

## Step 3: Figure out<br>where your data is

Need to find your data within the `myhtml` object. 

Tags to look for: 

- `<p>`: paragraphs
- `<h1>`, `<h2>`, etc.: headers
- `<a>`: links
- `<li>`: item in a list
- `<table>`: tables

Use [Selector Gadget](http://selectorgadget.com/) to find the exact location. (Demo)

For more on HTML, I recommend [W3schools' tutorial](http://www.w3schools.com/html/html_intro.asp) 
>- You don't need to be an expert in HTML to webscrape with `rvest`!

## Step 4: Tell `rvest` where to find your data

Copy-paste from Selector Gadget or give HTML tags into `html_nodes()` to extract your data of interest
```{r getdesc}
myhtml %>% html_nodes(".summary_text") %>% html_text()
```

```{r gettable}
myhtml %>% html_nodes("table") %>% html_table(header = TRUE)
```

## Step 5: Save & tidy data {.smaller}

```{r savetidy}
library(stringr)
library(magrittr)
mydat <- myhtml %>% 
  html_nodes("table") %>%
  extract2(1) %>% 
  html_table(header = TRUE)
mydat <- mydat[,c(2,4)]
names(mydat) <- c("Actor", "Role")
mydat <- mydat %>% 
  mutate(Actor = Actor,
         Role = str_replace_all(Role, "\n  ", ""))
mydat
```

## Your Turn #1

Using `rvest`, scrape a table from Wikipedia. You can pick your own table or you can get one of the tables in the country [GDP per capita](https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(PPP)_per_capita) example from earlier. 

Your result should be a data frame with one observation per row and one variable per column. 

## Your Turn #1 Solution

```{r yourturn1}
library(rvest)
library(magrittr)
myurl <- "https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(PPP)_per_capita"
myhtml <- read_html(myurl)
myhtml %>% 
 html_nodes("table") %>%
 extract2(3) %>%
 html_table(header = TRUE, fill = T) %>% 
 mutate(`Int$` = parse_number(`Int$`)) %>% 
 head
```

# Deeper dive into `rvest`

## Key Functions: `html_nodes`

- `html_nodes(x, "path")` extracts all elements from the page `x` that have the tag / class / id `path`. (Use SelectorGadget to determine `path`.) 
- `html_node()` does the same thing but only returns the first matching element. 
- Can be chained

```{r nodesex}
myhtml %>% 
  html_nodes("p") %>% # first get all the paragraphs 
  html_nodes("a") # then get all the links in those paragraphs
```

## Key Functions: `html_text`

- `html_text(x)` extracts all text from the nodeset `x` 
- Good for cleaning output

```{r textex}
myhtml %>% 
  html_nodes("p") %>% # first get all the paragraphs 
  html_nodes("a") %>% # then get all the links in those paragraphs
  html_text() # get the linked text only 
```

## Key Functions: `html_table` {.smaller}

- `html_table(x, header, fill)` - parse html table(s) from `x` into a data frame or list of data frames 
- Structure of HTML makes finding and extracting tables easy!

```{r tableex}
myhtml %>% 
  html_nodes("table") %>% # get the tables 
  head(2) # look at first 2
```

```{r tableex2}
myhtml %>% 
  html_nodes("table") %>% # get the tables 
  extract2(3) %>% # pick the second one to parse
  html_table(header = TRUE) # parse table 
```

## Key functions: `html_attrs`

- `html_attrs(x)` - extracts all attribute elements from a nodeset `x`
- `html_attr(x, name)` - extracts the `name` attribute from all elements in nodeset `x`
- Attributes are things in the HTML like `href`, `title`, `class`, `style`, etc.
- Use these functions to find and extract your data

```{r attrsex}
myhtml %>% 
  html_nodes("table") %>% extract2(2) %>%
  html_attrs()
```

```{r attrsex2}
myhtml %>% 
  html_nodes("p") %>% html_nodes("a") %>%
  html_attr("href")
```

## Other functions

- `html_children` - list the "children" of the HTML page. Can be chained like `html_nodes`
- `html_name` - gives the tags of a nodeset. Use in a chain with `html_children`

```{r childex}
myhtml %>% 
  html_children() %>% 
  html_name()
```

- `html_form` - parses HTML forms (checkboxes, fill-in-the-blanks, etc.)
- `html_session` - simulate a session in an html browser; use the functions `jump_to`, `back` to navigate through the page

## Your Turn #2

Find another website you want to scrape (ideas: [all bills in the house so far this year](https://www.congress.gov/search?q={%22congress%22:%22all%22,%22source%22:%22legislation%22,%22search%22:%22actionDateChamber:\%22115|H|2017-02-03\%22+AND+(billIsReserved:\%22N\%22+or+type:\%22AMENDMENT\%22)%22}&pageSort=documentNumber:asc), [video game reviews](http://www.metacritic.com/game), anything Wikipedia) and use *at least* 3 different `rvest` functions in a chain to extract some data.

# Advanced Examples:<br>Into the Weeds<br>![](http://mediamarathoning.com/wp-content/uploads/2013/02/In-the-weeds.jpg)

# Example #1: Inaugural Addresses

## The Data
- [The Avalon Project](http://avalon.law.yale.edu/subject_menus/inaug.asp) has most of the U.S. Presidential inaugural addresses. 
- Obama 2013, Trump 2017, VanBuren 1837, Buchanan 1857, Garfield 1881, and Coolidge 1925 are missing, but are easily found elsewhere. I have them saved as text files on Github
- Let's scrape all of them from The Avalon Project! 

## Get data frame of addresses

- Could use another source to get this data of President names and years of inaugurations, but we'll use The Avalon Project's site because it's a good example of data that needs tidying. 
```{r getyears}
url <- "http://avalon.law.yale.edu/subject_menus/inaug.asp"
# even though it's called "all inaugs" some are missing
all_inaugs <- (url %>% 
  read_html() %>% 
  html_nodes("table") %>% 
  html_table(fill=T, header = T)) %>% extract2(3)
# table of addresses
all_inaugs_tidy <- all_inaugs %>% 
  gather(term, year, -President) %>% 
  filter(!is.na(year)) %>% 
  select(-term) %>% 
  arrange(year)
head(all_inaugs_tidy)
```

## Get links to visit & scrape

```{r getlinks}
# get the links to the addresses 
inaugadds_adds <- (url %>%
  read_html() %>%
  html_nodes("a") %>%
  html_attr("href"))[12:66]
# create the urls to scrape
urlstump <- "http://avalon.law.yale.edu/"
inaugurls <- paste0(urlstump, str_replace(inaugadds_adds, "../", ""))
all_inaugs_tidy$url <- inaugurls
head(all_inaugs_tidy)
```

## Automate scraping

- A function to read the addresses and get the text of the speeches, with a catch for a read error
```{r functiongetspeech, cache=TRUE}
get_inaugurations <- function(url){
  test <- try(url %>% read_html(), silent=T)
  if ("try-error" %in% class(test)) {
    return(NA)
  } else
    url %>% read_html() %>%
      html_nodes("p") %>% 
      html_text() -> address
    return(unlist(address))
}

# takes about 30 secs to run
all_inaugs_text <- all_inaugs_tidy %>% 
  mutate(address_text = (map(url, get_inaugurations))) 

all_inaugs_text$address_text[[1]]
```

## Add Missings

```{r missings}
all_inaugs_text$President[is.na(all_inaugs_text$address_text)]
# there are 7 missing at this point: obama's and trump's, plus coolidge, garfield, buchanan, and van buren, which errored in the scraping.
obama09 <- get_inaugurations("http://avalon.law.yale.edu/21st_century/obama.asp")
obama13 <- readLines("speeches/obama2013.txt")
trump17 <- readLines("speeches/trumpinaug.txt")
vanburen1837 <- readLines("speeches/vanburen1837.txt") # row 13
buchanan1857 <- readLines("speeches/buchanan1857.txt") # row 18
garfield1881 <- readLines("speeches/garfield1881.txt") # row 24
coolidge1925 <- readLines("speeches/coolidge1925.txt") # row 35
all_inaugs_text$address_text[c(13,18,24,35)] <- list(vanburen1837,buchanan1857, garfield1881, coolidge1925)

# lets combine them all now
recents <- data.frame(President = c(rep("Barack Obama", 2), 
                                    "Donald Trump"),
                      year = c(2009, 2013, 2017), 
                      url = NA,
                      address_text = NA)

all_inaugs_text <- rbind(all_inaugs_text, recents)
all_inaugs_text$address_text[c(56:58)] <- list(obama09, obama13, trump17)
```

## Check-in: What did we do?

1. We found some interesting data to scrape from the web.
2. We used tidy tools to create tidy data:
    + A data frame of President and year. One observation per row!
    + Stored urls we wished to scrape with their data
    + Stored the scraped speech with the matching President, year, and url
3. We used the consistent HTML structure of the urls we wanted to scrape to automate collection of web data
    + Way faster than copy-paste! 
    + Though  we had to do some by hand, we took advantage of the tidy data and added the missing data manually without much pain.
4. We now have a tidy data set of Presidential inaugural addresses for text analysis!
    + Each variable forms a column
    + Each observation forms a row
    + Each type of observational unit forms a table 
    
## A (Small) Text Analysis

Now, I use the [`tidytext`](http://tidytextmining.com/) package to get the words out of each inaugural address. 

```{r textanalysis}
# install.packages("tidytext")
library(tidytext)
all_inaugs_text %>% 
  select(-url) %>% 
  unnest() %>% 
  unnest_tokens(word, address_text) -> presidential_words
head(presidential_words)
```

## Longest speeches

```{r longestspeech}
presidential_words %>% 
  group_by(President,year) %>% 
  summarize(num_words = n()) %>%
  arrange(desc(num_words)) -> presidential_wordtotals
```
```{r speechplot, echo = FALSE, fig.align='center', cache = T, fig.height=5, fig.width=9.5}
library(ggrepel)
ggplot(presidential_wordtotals) + 
  geom_bar(aes(x = reorder(year, num_words), y = num_words), stat = "identity", fill = 'white', color = 'black') + 
  geom_label_repel(aes(x = reorder(year, num_words), y = num_words, label = President), color = "#88398A", size = 2.5) +
  labs(y = "Word count of Speech", x = "Year (sorted by word count)", title = "Length of Presidential Inaugural Addresses", subtitle = "Max: 8,459; Min: 135; Median: 2,090; Mean: 2,341") + 
  r_ladies_theme() + theme(axis.text.x = element_text(angle = 45, size = 7), plot.subtitle = element_text(hjust = .5))
```

# Example #2: Notable Deaths<br>![](http://25.media.tumblr.com/tumblr_me2hwiAYzB1qlhqyjo2_250.png)![](http://i2.cdn.cnn.com/cnnnext/dam/assets/160421193148-prince-large-169.jpg)<br>![](http://mediamass.net/jdd/public/documents/celebrities/3739.jpg)![](http://vignette1.wikia.nocookie.net/doblaje/images/a/a3/5733-30568-0.jpg/revision/latest?cb=20160426213339&path-prefix=es)  

## The Data

- 2016 felt to many people like a year of loss: David Bowie, Prince, Alan Rickman, Carrie Fisher, and many more celebrities passed away in 2016
- But were there really more "celebrity deaths" than any other year? 
- [Wikipedia](https://en.wikipedia.org/wiki/2016#Deaths) has a list of notable deaths every year, going all the way back to 1987.
- We can scrape Wikipedia pages for this data.

## Scraping Wikipedia

First, get all the URLs for the Wikipedia articles for the years of 1987-2016.
```{r scrapingtrial}
years <- 1987:2016
urls <- paste0("https://en.wikipedia.org/wiki/", years, "#Deaths")
```

Next, create a data frame to store all of the data.

```{r dataframewiki}
celebDeaths <- data.frame(year = years, url = urls,
                          stringsAsFactors = FALSE)
```

## Look at the HTML

```{r readyear}
urls[1] %>% read_html() %>% html_children() %>%
  html_nodes("h2")
urls[1] %>% read_html() %>% html_children() %>%
  html_nodes("li")
```

## Start Scraping

- Write a function for scraping all the years, just like with the Presidents' inaugural addresses
- Unfortunately, the lists aren't as structured as the Wikipedia table
- This creates some difficulties...
- But, luckily, the same exact difficulties exist on each page, so we only have to deal with them once!

## Write the function (1/2)

- Heads up - this is a difficult example. Don't worry if you don't understand everything right away
- Also, this is not a unique solution to this problem 

```{r cdfunc1, echo = FALSE}
get_deaths <- function(url){
  # get the main content page
  page <- url %>% read_html() %>% 
    html_nodes("#mw-content-text") %>% html_children() %>% 
    html_children()
  # get the names of all elements 
  tagnames <- page %>% html_name()
  # where are the big section headers
  h2s <- which(tagnames == "h2")
  # to find the heading labeled "Deaths"
  h2childids <- page[h2s] %>% html_children() %>% html_attr("id")
  idDeaths <- which(h2childids == "Deaths")
  # list of deaths starts after the location of deathStart and 
  # ends immediately before the location of deathEnd (next big header)
  deathStart <- h2s[(idDeaths+1)/2]
  deathEnd <- h2s[(idDeaths+1)/2+1]
  # get the deaths
  death_elements <- page[(deathStart+1):(deathEnd-1)] 
  deaths <- death_elements %>% html_nodes("li") %>% html_text()
  # there are two types of deaths: there was only one death that day in that year (a)
  deathsa <- data.frame(death = deaths[grep("–", deaths)])
  deathsa <- deathsa %>% separate(death, into = c("Date", "Person"), sep = " – ") %>% 
    separate(Date, into = c("Month", "Day"), sep = " ") %>%
    separate(Person, into = c("Name", "Desc"), sep = ", ", extra = "merge") 
  # or there were multiple deaths that day in that year 
  deathsb <- data.frame(death = deaths[-grep("–", deaths)], stringsAsFactors = F)
  deathsb <- data.frame(death = deathsb[grep("\n",deathsb$death),], stringsAsFactors = F)
  deathsb %>% separate(death, into = c("Date", "Other"), sep = "\\n", extra="merge") %>%
    separate(Other, into = paste0("Person", 1:6), sep = "\\n", fill = "right") %>% 
    gather(Person, Desc, -Date) %>% 
    select(Date, Desc) %>%
    filter(!is.na(Desc)) -> deathsb
  
  deathsb %>% separate(Desc, into = c("Name", "Desc"), sep = ", ", extra = "merge") %>%
    separate(Date, into = c("Month", "Day"), sep = " ") %>%
    filter(!is.na(Desc)) -> deathsb
  
  deaths <- rbind(deathsa, deathsb)

  return(deaths)
}
```
```{r cdfunc, eval=FALSE}
get_deaths <- function(url){
  # get the main content page
  page <- url %>% read_html() %>% 
    html_nodes("#mw-content-text") %>% html_children() %>% 
    html_children()
  # get the names of all elements 
  tagnames <- page %>% html_name()
  # where are the big section headers
  h2s <- which(tagnames == "h2")
  # to find the heading labeled "Deaths"
  h2childids <- page[h2s] %>% html_children() %>% html_attr("id")
  idDeaths <- which(h2childids == "Deaths")
  # list of deaths starts after the location of deathStart and 
  # ends immediately before the location of deathEnd (next big header)
  deathStart <- h2s[(idDeaths+1)/2]
  deathEnd <- h2s[(idDeaths+1)/2+1]
  # get the deaths
  death_elements <- page[(deathStart+1):(deathEnd-1)] 
  deaths <- death_elements %>% html_nodes("li") %>% html_text()
```
(continued on next slide)

## Write the function (2/2)

```{r func2, eval=FALSE}
# there are two types of deaths: there was only one death that day in that year (a)
  deathsa <- data.frame(death = deaths[grep("–", deaths)])
  deathsa <- deathsa %>% 
    separate(death, into = c("Date", "Person"), sep = " – ") %>% 
    separate(Date, into = c("Month", "Day"), sep = " ") %>%
    separate(Person, into = c("Name", "Desc"), sep = ", ", extra = "merge") 
  # or there were multiple deaths that day in that year (b) 
  deathsb <- data.frame(death = deaths[-grep("–", deaths)], stringsAsFactors = F)
  # remove repeats
  deathsb <- data.frame(death = deathsb[grep("\n",deathsb$death),], stringsAsFactors = F)
  # tidy up the data
  deathsb %>% 
    separate(death, into = c("Date", "Other"), sep = "\\n", extra="merge") %>%
    separate(Other, into = paste0("Person", 1:6), sep = "\\n", fill = "right") %>% 
    gather(Person, Desc, -Date) %>% 
    select(Date, Desc) %>%
    filter(!is.na(Desc)) -> deathsb
  deathsb %>% separate(Desc, into = c("Name", "Desc"), sep = ", ", extra = "merge") %>%
    separate(Date, into = c("Month", "Day"), sep = " ") %>%
    filter(!is.na(Desc)) -> deathsb
  #combine the 2 sets
  deaths <- rbind(deathsa, deathsb)

  return(deaths)
} 
```

## Use the function!

- Use the same tidy principles we used for the inaugural example. 
```{r getalldeaths, cache = TRUE}
# should take about 10 seconds
celebDeaths <- celebDeaths %>% 
  mutate(Deaths = map(url, get_deaths)) %>%
  unnest()
head(celebDeaths[,-2])
```

## Check-in: What did we do?

1. We found some interesting data to scrape from the web.
2. We used tidy tools to create tidy data:
    + Years and Wikipedia pages associated with them
    + Stored the scraped data with the matching year and URL
3. We spent some time decoding the HTML & figuring out how to find where our data was stored
    + Struggled with lack of structure in the lists we wanted 
    + Not a unique solution
4. Wrote a function to scrape a page; applied it to each year in our data
5. Output: A tidy data frame of one person per row with dates, names, and descriptions

## A (Small) Data Analysis

- We want to know if 2016 really was a very significant year of celebrity deaths
- Let's get a quick count

```{r countdeaths}
celebDeaths %>% 
  group_by(year) %>% 
  summarise(num_deaths = n()) %>% 
  arrange(desc(num_deaths)) %>% 
  head(10)
```

## Over time?

- Some [people](http://www.bbc.com/news/magazine-38329740) have postulated that there is an increase in deaths because we are 50+ years out from the cultural revolution of the 1960s.
- Let's see if there's a trend over time: 
```{r timetrend, echo = FALSE, fig.align='center',fig.height=5, fig.width=8.5}
celebDeaths %>% 
  group_by(year) %>% 
  summarise(num_deaths = n()) %>% 
  arrange(desc(num_deaths)) -> cdsummary
ggplot(data = cdsummary, aes(x = year, y = num_deaths)) + 
  geom_line() + 
  geom_smooth(color = "#562457", alpha = .5, linetype = "dashed", size = .5) + 
  geom_point(inherit.aes = TRUE, color = "#88398A") + 
  labs(x = "Year", y = 'Number of "Celebrity Deaths"', title = "Celebrity Deaths over Time", subtitle = "(Celebrities according to Wikipedia)") + 
  r_ladies_theme() + theme(plot.subtitle = element_text(hjust=.5))
```

# Conclusion

## What did we do?

>- Learned about webscraping and why you'd want to do it
>- Saw some resources for webscraping in `R`
>- Got to know the `tidyverse`
>- Scraped data from the web with `rvest`
>- Discovered the longest inaugural address given by a US President was over 8,000 words
>- Found out that 2016 really was a major year in celebrity deaths
>- Had fun!

## Thank you!<br>![](https://media.giphy.com/media/jYAGkoghdmD9S/giphy.gif)

>- Questions? We have the room until 6pm!
